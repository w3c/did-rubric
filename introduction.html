<h2>Introduction</h2>

<section>
    <h3>Background</h3>
    <p>
        A rubric is a tool used in academia to communicate expectations and evaluate
        performance. It consists of a set of criteria to be evaluated, possible
        responses for each criterion, and a scoring guide explaining both how to
        choose and interpret each response. The act of evaluating a rubric, which we
        call an Evaluation, provides basis for self-evaluation, procurement decisions,
        or even marketing points. Written records of
        an evaluation, which we'll call an <a>Evaluation Report</a>, document how a
        particular subject is measured against the criteria. For students, a rubric
        helps to clarify how their work will be evaluated by others. For
        <a>Evaluator</a>s, a rubric provides a consistent framework for investigating
        and documenting the performance of a subject against a particular set of
        criteria.
    </p>
    <p>
        We were inspired to develop a rubric for
        decentralization when discussions about the requirements for decentralized
        identifiers, aka DIDs, led to intractable disagreement. It became clear that
        no single definition of "decentralized" would suffice for all of the
        motivations that inspired developers of DID Methods to work on a new
        decentralized approach to identity. Despite this challenge, two facts remained
        clear:
    </p>
    <ol>
        <li>
            the people invested in this work shared a common goal of reversing the
            problems with centralized identity systems and
        </li>
        <li>
            they also had numerous, distinct reasons for doing so.
        </li>
    </ol>
    <p>
        Rather than attempt to force a definition of "decentralized" that might work
        for many but would alienate others, the group set out to capture the
        measurable factors that could enable <a>Evaluator</a>s to judge the
        decentralization of DID Methods based on their own independent requirements,
        with minimal embedded bias.
    </p>
</section> <!-- background -->

<section>
    <h3>How to apply this rubric</h3>
    <p>
        Pick the most important criteria for your use, ask each question, and select the
        most appropriate response. Do this for all of the DID
        Methods under consideration.
    </p>
    <p>
        Each <a>Evaluation</a> should start with an explicit framing of the use under
        consideration. Are you evaluating the Method for use in Internet-of-Things
        (IoT)? For school childrens' extra-curricular activities? For international
        travel? The use, or set of uses, will directly affect how some of the
        questions are answered.
    </p>
    <p>
        Where a given Method offers variability, such as multiple networks for the
        same Method, then evaluate each variant. For example, did:ethr supports
        Ethereum mainnet, multiple testnets and permissioned EVM-compliant networks
        such as Quorum. To apply a criterion to did:ethr, you will evaluate it against
        all the variations that matter <em>to you</em>. Each variation should get its
        own <a>Evaluation</a>. This applies to Level 2 Networks that can operate on
        multiple Level 1 Networks as well as DID Methods that directly offer support
        for multiple underlying DID registries.
    </p>
    <p>
        When creating an <a>Evaluation Report</a>, we recommend noting both the
        <a>Evaluator</a> and the date of the <a>Evaluation</a>. Many of the criteria
        are subjective and all of them may evolve. Tracking who
        made the <a>Evaluation</a> and when they made it will help readers better
        understand any biases or timeliness issues that may affect the applicability
        of the <a>Evaluation</a>.
    </p>
    <p>
        Be selective and acknowledge the subjective. <a>Evaluations</a> do not need to
        be exhaustive. There is no requirement to answer all the questions. Simply
        answer the ones most relevant to the use contemplated. Similarly, understand
        that any recorded <a>Evaluation</a> is going to represent the biases of the
        <a>Evaluator</a> in the context of their target use. Even the same
        <a>Evaluator</a>, evaluating the same Method for a different use, may come up
        with slightly different answers&mdash;for example, that which is economically
        accessible for small businesses might not be cost-effective for refugees, and
        that could affect how well-suited a given Method is for a specific use.
    </p>
    <p>
        Finally, note that this particular rubric is about decentralization. It
        doesn't cover all of the other criteria that might be relevant to evaluating a
        given DID Method. There are security, privacy, and economic concerns that
        should be considered. We look forward to working with the community to develop
        additional rubrics for these other areas and encourage <a>Evaluator</a>s to
        use this rubric as a starting point for their own work rather than the final
        say in the merit of any given Method.
    </p>
    <p>
        In short, use this rubric to help understand if a given DID Method is
        decentralized enough for your needs.
    </p>

</section> <!-- how to apply -->

<section>
    <h3>Evaluation reports</h3>
    <p>
        To record and report an <a>Evaluation</a>, we recommend two possible formats,
        either comprehensive or comparative.
    </p>
    <p>
        A comprehensive <a>Evaluation</a> applies a single set of criteria to just one
        Method. This set is chosen by the <a>Evaluator</a>; it need not be all
        possible criteria, but it is all relevant criteria as judged by the
        <a>Evaluator</a>. This is the type of <a>Evaluation Report</a> we include in
        <a href="#"></a>.
    </p>
    <p>
        A comparative <a>Evaluation</a> includes multiple Methods in the same table to
        easily compare and contrast two or more different Methods. This may include
        any number of criteria. These are the type of reports we use as examples
        throughout the criteria list.
    </p>
    <p>
        In addition to the selected criteria, we recommend each report specify
    </p>
    <ol>
        <li>
            The Method(s) being evaluated
        </li>
        <li>
            A link to the Method specification
        </li>
        <li>
            The <a>Evaluator</a>(s)
        </li>
        <li>
            The date of the <a>Evaluation</a>
        </li>
        <li>
            A description of the use case(s) for which the Method is being evaluated
        </li>
        <li>
            The rubric used for the <a>Evaluation</a>, along with reference to the
            specific version.
        </li>
        <li>
            Optionally, a URL for retrieving the report.
        </li>
    </ol>
    <p>
        An example comprehensive report header can be found in <a href="#"></a>. An
        example comparative report header can be found in
        <a href="#comparative-evaluation-report-header"></a>.
    </p>
</section> <!-- evaluation -->

<section>
    <h3>Categories of criteria</h3>
    <p>
        We have grouped our criteria into five categories:
    </p>
    <ol>
        <li>
            Rulemaking
        </li>
        <li>
            Operations
        </li>
        <li>
            Enforcement
        </li>
        <li>
            Alternatives
        </li>
        <li>
            Adoption
        </li>
    </ol>

    <p>
        <a>Evaluator</a>s should consider criteria from all five groups, as best fits
        your use cases. The first three are about how a given Method is governed:
        Rulemaking, Operations, and Enforcement. The latter two address issues of
        lock-in and accessibility: Alternatives and Adoption.
    </p>
    <p>
        A few notes about governance. Our approach parallels the same breakdown in
        authority embodied in the United States Constitution.
    </p>
    </p>
    <p>
        <strong>Rulemaking</strong> addresses who makes the rules and how. (This is
        the legislative branch in the US.)
    </p>
    <p>
        <strong>Operations</strong> addresses how those rules are executed and how
        everyone knows that they are carried out. (This is the executive branch in the
        US.)
    </p>
    <p>
        <strong>Enforcement</strong> addresses how we find and respond to rule
        breaking. (This is the judicial branch in the US.)
    </p>
    <p>
        This mental model is key to understanding the criteria of each section as well
        as why we included some criteria and not others.
    </p>
    <p>
        The sections on Alternatives and Adoption were created because we identified
        decentralization factors that have nothing to do with governance. As an
        example, a completely open system of formal "decentralized" governance could
        be <em>de facto</em> centralized because it is the only available option. If
        there is only one wallet you can use for a given Method, that may
        be unacceptably centralized for a given use. On the other hand, if that wallet
        is already ubiquitous in the use cases that matter, this centralizing factor
        may not be as relevant given its accessibility by intended users. Similarly,
        some use cases would be dramatically limited if there were only one relying
        party who is willing to accept DIDs of a given Method. If a theoretical
        did:facebook Method were only accepted by Facebook (because no other services were
        willing to use it) that would affect its centralization, even if it were possible
        by design for any service to do so. The criteria in Alternatives and Adoption
        sections attempt to evaluate these factors.
    </p>
</section> <!-- categories -->

<section>
    <h3>The architecture</h3>
    <p>
        â€‹When evaluating the governance of DID Methods, three potentially independent
        layers should be considered: the specification, the network, and the registry.
    </p>
    <ul>
        <li>
            The <strong>specification</strong> is the governing document for the Method
            that outlines how that particular Method implements the required and any
            optional components of the DID Core specification [[DID-CORE]].
        </li>
        <li>
            The <strong>network</strong> is the underlying communications layer, i.e., how
            users of the Method communicate with others to invoke the operations of the
            Method.
        </li>
        <li>
            The <strong>registry</strong> is a given instance of recorded state changes,
            managed according to the specification, using the communications channel.
        </li>
    </ul>
    <p>
        For Rulemaking, the criteria should be evaluated against all three of the
        above layers.
    </p>
    <p>
        For Operations, the criteria should be evaluated against the network and the
        registry. The specification is taken as a given (it is the core output of
        Rulemaking).
    </p>
    <p>
        For Adoption, the criteria should be evaluated for each major software
        component: wallet, resolver, and registry.
    </p>
    <p>
        For Alternatives, the criteria should be evaluated against the particular DID
        Method.
    </p>
    <p>
        For the examples in the rest of this document we refer to a set of Methods
        that are familiar to the authors and exhibit interesting characteristics for
        <a>Evaluation</a>. These are listed in the "Comparative Evaluation Report
        Header" table below.
    </p>
</section> <!-- architecture -->

<section>
    <h3>Comparative evaluation report header</h3>
    <table class="simple">
        <caption>
            Comparison Evaluation (used throughout the criteria list)
        </caption>
        <tr>
            <th>
                <a>Evaluator</a>s
            </th>
            <td>
                Joe Andrieu &lt;<a href="mailto:joe@legreq.com">joe@legreq.com</a>&gt;
            </td>
        </tr>
        <tr>
            <th>
                Evaluation date
            </th>
            <td>
                2020-01-03
            </td>
        </tr>
        <tr>
            <th>
                Use cases
            </th>
            <td>
                <p>
                    <strong>Verifiable software development.</strong> The signing of commits by
                    developers and their verification to ensure that source code in a particular
                    git repository is authentic.
                </p>
                <p>
                    <strong>User authentication.</strong> The use of DIDs for authenticating users
                    for access to system services.
                </p>
                <p>
                    <strong>Long term verifiable credentials.</strong> The use of DIDs as subject
                    identifiers for long term (life-long) verifiable credentials such as earned
                    academic degrees.
                </p>
            </td>
        </tr>
        <tr>
            <th>
                Report URL
            </th>
            <td>
                TBD
            </td>
        </tr>
        <tr>
            <th>
                Rubric
            </th>
            <td>
                A Rubric for Decentralization of DID Methods v0.0.1
            </td>
        </tr>
        <tr>
            <th>
                Rubric URL
            </th>
            <td>
                TBD
            </td>
        </tr>
    </table>

    <table class="simple">
        <caption>
            <p>Methods considered</p>
        </caption>
        <tr>
            <th>
                Method
            </th>
            <th>
                Specification
            </th>
            <th>
                Network
            </th>
            <th>
                Registry
            </th>
        </tr>
        <tr>
            <td>
                did:peer
            </td>
            <td>
                <a href="https://identity.foundation/peer-did-method-spec/index.html">Peer DID
                    Method Spec</a>
            </td>
            <td>
                n/a (communications can flow over any agreeable channel)
            </td>
            <td>
                Held by each peer
            </td>
        </tr>
        <tr>
            <td>
                did:git
            </td>
            <td>
                <a href="https://github.com/dhuseby/did-git-spec/blob/master/did-git-spec.md">
                    DID git Spec</a>
            </td>
            <td>
                <a href="https://git-scm.com/">git</a> (an open source version control system)
            </td>
            <td>
                Any Method-compliant git repository
            </td>
        </tr>
        <tr>
            <td>
                did:btcr
            </td>
            <td>
                <a href="https://w3c-ccg.github.io/didm-btcr">DID Method BTCR</a>
            </td>
            <td>
                Bitcoin
            </td>
            <td>
                Bitcoin
            </td>
        </tr>
        <tr>
            <td>
                did:sov
            </td>
            <td>
                <a href="https://sovrin-foundation.github.io/sovrin/spec/did-Method-spec-template.html">
                    Sovrin DID Method Spec</a>
            </td>
            <td>
                Hyperledger Indy
            </td>
            <td>
                A particular instance of Hyperledger Indy
            </td>
        </tr>
        <tr>
            <td>
                did:ethr
            </td>
            <td>
                <a href="https://github.com/decentralized-identity/ethr-did-resolver/blob/develop/doc/did-method-spec.md">
                    ethr DID Method Spec</a>
            </td>
            <td>
                Ethereum
            </td>
            <td>
                Specific smart contracts for each network.
            </td>
        </tr>
        <tr>
            <td>
                did:jolo
            </td>
            <td>
                <a href="https://github.com/jolocom/jolocom-did-driver/blob/master/jolocom-did-Method-specification.md">
                    Jolocom DID Method Specification</a>
            </td>
            <td>
                Ethereum
            </td>
            <td>
                Specific smart contracts for different networks and subnetworks.
            </td>
        </tr>
    </table>
    <p class="note">
        Note: The evaluations in this document are made by Joe Andrieu
        &lt;joe@legreq.com&gt;. Other authors have expressed different evaluations,
        because of different weighting of the relevant of different characteristics
        in particular methods. This should be expected in any evaluation: they will
        always be colored by the particular perspective of the <a>Evaluator</a>.
    </p>
</section> <!-- evaluation report header -->
